import gradio as gr
import chromadb
import requests
import json
import os

# ================= è¨­å®šå€ =================
# è³‡æ–™åº«è¨­å®š
DB_PATH = "./local_vector_db"
COLLECTION_NAME = "ot_reports"

# Ollama è¨­å®š (ç”¨æ–¼ Embedding å’Œç”Ÿæˆ)
OLLAMA_API_URL = "http://localhost:11434/api"
EMBEDDING_MODEL = "nomic-embed-text"  # å¿…é ˆèˆ‡å»ºç«‹è³‡æ–™åº«æ™‚ä¸€è‡´
GENERATION_MODEL = "qwen2.5:7b"      # ç”Ÿæˆç”¨çš„æ¨¡å‹ï¼Œå¯æ›æˆ llama3 æˆ–å…¶ä»–
# =========================================

# 1. è³‡æ–™åº«é€£ç·šå‡½å¼
def get_chroma_collection():
    client = chromadb.PersistentClient(path=DB_PATH)
    return client.get_collection(COLLECTION_NAME)

# 2. Embedding å‡½å¼ (å°‡æ–‡å­—è½‰å‘é‡)
def get_embedding(text):
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/embeddings",
            json={"model": EMBEDDING_MODEL, "prompt": text},
            timeout=10
        )
        if response.status_code == 200:
            return response.json()["embedding"]
        else:
            print(f"Embedding Error: {response.text}")
            return None
    except Exception as e:
        print(f"Ollama Connection Error: {e}")
        return None

# 3. ç”Ÿæˆå›æ‡‰å‡½å¼ (RAG æ ¸å¿ƒé‚è¼¯)
def generate_report(case_description):
    
    status_msg = "æ­£åœ¨åˆ†æè³‡æ–™..."
    yield status_msg # å›å‚³çµ¦å–®ä¸€è¼¸å‡ºæ¬„ä½
    
    # æŸ¥è©¢æ–‡å­—ç›´æ¥ä½¿ç”¨è¼¸å…¥å…§å®¹
    query_text = case_description
    
    # --- æ­¥é©Ÿ A: æª¢ç´¢ (Retrieval) ---
    embedding = get_embedding(query_text)
    if not embedding:
        err_msg = "éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥ Ollamaï¼Œè«‹ç¢ºèª Ollama å·²å•Ÿå‹•ã€‚"
        yield err_msg
        return

    collection = get_chroma_collection()
    # ç‚ºäº†ç¢ºä¿èƒ½æ•æ‰åˆ°ã€Œè©•ä¼°å·¥å…·ã€èˆ‡ã€Œç”Ÿæ´»è‡ªç†ã€çš„é«˜åº¦ç›¸é—œè³‡æ–™ï¼Œå…ˆå–å‰ 5 ç­†å†ä¾†éæ¿¾
    results = collection.query(
        query_embeddings=[embedding],
        n_results=5 
    )
    
    status_msg += "\nå·²æª¢ç´¢åˆ°åƒè€ƒæ¡ˆä¾‹ï¼Œæ­£åœ¨é€²è¡Œé—œè¯åº¦ç¯©é¸..."
    yield status_msg

    # æ•´ç†åƒè€ƒè³‡æ–™å­—ä¸² (åš´æ ¼éæ¿¾ï¼šç›¸ä¼¼åº¦ > 0.6)
    filtered_docs = []
    if results['distances'] and results['documents']:
        for i, dist in enumerate(results['distances'][0]):
            similarity = 1.0 - dist
            if similarity > 0.6:
                doc = results['documents'][0][i]
                filtered_docs.append(f"ã€åƒè€ƒæ¡ˆä¾‹ {i+1} (é—œè¯åº¦: {similarity:.2f})ã€‘\n{doc}\n")
    
    if not filtered_docs:
        context_str = "ï¼ˆâš ï¸ è­¦å‘Šï¼šè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ > 0.6 çš„é«˜ç›¸é—œæ¡ˆä¾‹ï¼Œä»¥ä¸‹å ±å‘Šå°‡åƒ…åŸºæ–¼ä¸€èˆ¬è·èƒ½æ²»ç™‚åŸå‰‡ç”Ÿæˆï¼Œå¯èƒ½ä¸å¤ ç²¾æº–ï¼‰"
        status_msg += "\nâš ï¸ æœªæ‰¾åˆ°é«˜ç›¸é—œæ¡ˆä¾‹ (>0.6)ï¼Œåƒ…ä¾ä¸€èˆ¬é‚è¼¯ç”Ÿæˆã€‚"
    else:
        context_str = "\n".join(filtered_docs)
        status_msg += f"\nç¯©é¸å®Œæˆï¼Œæ‰¾åˆ° {len(filtered_docs)} ç­†é«˜ç›¸é—œæ¡ˆä¾‹ (>0.6)ã€‚ç”Ÿæˆå ±å‘Šä¸­..."
    
    yield status_msg
        
    # --- æ­¥é©Ÿ B: ç”Ÿæˆ (Generation) ---
    system_prompt = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è·èƒ½æ²»ç™‚å¸« (OT)ã€‚
ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šã€Œä½¿ç”¨è€…æä¾›çš„ä¸»è¨´èˆ‡è§€å¯Ÿã€ï¼Œé«˜åº¦ä¾è³´ã€Œæ­·å²æ¡ˆä¾‹çš„åˆ†æé‚è¼¯ã€èˆ‡ã€Œè³‡æ–™åº«è©å½™ã€ï¼Œæ’°å¯«ä¸€ä»½å°ˆæ¥­çš„ã€å•é¡Œåˆ†æã€‘èˆ‡ã€æ²»ç™‚å»ºè­°ã€‘ã€‚

è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
1. **å¿…é ˆå…¨ç¨‹ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡ (Traditional Chinese, Taiwan)ã€‚**
2. **åè©è¦ç¯„**ï¼šå°ˆæœ‰åè©è«‹åš´æ ¼ä½¿ç”¨åƒè€ƒæ¡ˆä¾‹ä¸­å‡ºç¾çš„è³‡æ–™åº«åè© (ä¾‹å¦‚ï¼šæœ¬é«”è¦ºã€è§¸è¦ºé˜²ç¦¦ã€ç²¾ç´°å‹•ä½œ)ï¼Œä¸è¦è‡ªè¡Œå‰µé€ æˆ–ä½¿ç”¨ä¸ç†Ÿæ‚‰çš„åˆ¥åã€‚
3. **åˆ†æé‚è¼¯**ï¼šè«‹æ¨¡ä»¿åƒè€ƒæ¡ˆä¾‹çš„ã€Œè‡¨åºŠæ¨ç†è·¯å¾‘ã€ï¼Œä¸è¦æ†‘ç©ºç™¼æ®ã€‚ä¾‹å¦‚ï¼šåƒè€ƒæ¡ˆä¾‹å¦‚ä½•å°‡ã€Œåä¸ä½ã€é€£çµåˆ°ã€Œå‰åº­è¦ºã€ï¼Œä½ å°±è¦æ²¿ç”¨æ­¤é‚è¼¯ã€‚
4. **å»ºè­°é™åˆ¶**ï¼šç”¢å‡ºçš„å»ºè­°å…§å®¹èˆ‡èªæ„ï¼Œè«‹å‹¿åé›¢è³‡æ–™åº«è³‡æ–™çš„ç¯„ç–‡ã€‚
5. ç›´æ¥è¼¸å‡ºå ±å‘Šå…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµèªã€‚"""

    user_prompt = f"""
è«‹åƒè€ƒä»¥ä¸‹ã€Œé«˜åº¦ç›¸é—œã€çš„æ­·å²æ¡ˆä¾‹ï¼ˆè‹¥ç„¡ç›¸é—œæ¡ˆä¾‹å‰‡è«‹ä¿å®ˆåˆ†æï¼‰ï¼š
{context_str}

--------------------------------------------------
ã€ç›®å‰å€‹æ¡ˆè³‡æ–™ã€‘
{case_description}

è«‹æ ¹æ“šä¸Šè¿°åƒè€ƒè³‡æ–™çš„é‚è¼¯ï¼Œæ’°å¯«ï¼š
1. ### å•é¡Œåˆ†æ (è«‹ä½¿ç”¨è³‡æ–™åº«ä¸­çš„åˆ†æé‚è¼¯ï¼Œä¸¦ä»¥ã€Œæ¢åˆ—å¼ã€å‘ˆç¾ï¼Œæ•˜è¿°è«‹ç²¾ç°¡æ‰¼è¦)
2. ### ç¸½çµèˆ‡å»ºè­° (ç¬¬ä¸€é»å‹™å¿…ç‚ºã€Œç™‚è‚²èª²ç¨‹å»ºè­°ã€ï¼Œè«‹æ¨¡ä»¿è³‡æ–™åº«èªæ°£ï¼Œä¾‹å¦‚ï¼šã€Œç¶œåˆä»¥ä¸Šçµæœï¼Œå»ºè­°æŒçºŒè·èƒ½ç™‚è‚²èª²ç¨‹ã€ã€‚å¾ŒçºŒè«‹åˆ—å‡ºå„ç´°é …å»ºè­°ï¼Œä¸¦æ·±å…¥åƒè€ƒè³‡æ–™åº«å…§å®¹ï¼Œæ•˜è¿°å‹™å¿…è©³ç›¡å…·é«”ï¼Œé¿å…éæ–¼ç°¡ç•¥)
"""

    # å‘¼å« Ollama ç”Ÿæˆ (æ”¯æ´ä¸²æµé¡¯ç¤º)
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/chat",
            json={
                "model": GENERATION_MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": True # é–‹å•Ÿä¸²æµ
            },
            stream=True
        )

        full_response = ""
        for line in response.iter_lines():
            if line:
                # å¿…é ˆå…ˆ decode byte string
                decoded_line = line.decode('utf-8')
                try:
                    body = json.loads(decoded_line)
                    if "message" in body:
                        token = body["message"]["content"]
                        full_response += token
                        yield full_response # æ›´æ–°è¼¸å‡º
                    if body.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue
                    
    except Exception as e:
        err = f"ç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        yield err

# ================= ä»‹é¢è¨­è¨ˆ (Gradio) =================
with gr.Blocks(title="AI è·èƒ½æ²»ç™‚å ±å‘ŠåŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¥ AI è·èƒ½æ²»ç™‚å ±å‘ŠåŠ©æ‰‹ (Local RAG)")
    gr.Markdown("è¼¸å…¥å€‹æ¡ˆçš„ä¸»è¨´èˆ‡è§€å¯Ÿï¼Œå°‡åƒè€ƒæ­·å²ç—…æ­·åº«ï¼Œç”Ÿæˆå•é¡Œåˆ†æèˆ‡å»ºè­°ã€‚")
    
    with gr.Row():
        with gr.Column(scale=1):
            input_case = gr.Textbox(
                label="ä¸»è¨´èˆ‡è©•ä¼°å…§å®¹", 
                placeholder="ä¾‹å¦‚ï¼šå®¶å±¬è¡¨ç¤ºå­©å­åœ¨å­¸æ ¡åä¸ä½ï¼Œå¯«å­—å¾ˆé†œ... è§€å¯Ÿç™¼ç¾æŠ“æ¡å§¿å‹¢ä¸æˆç†Ÿï¼Œç„¡æ³•å–®è…³ç«™ç«‹...",
                lines=15
            )
            btn_submit = gr.Button("ğŸ§  é–‹å§‹ç”Ÿæˆå ±å‘Š", variant="primary")
            
        with gr.Column(scale=1):
            # ä½¿ç”¨ Markdown å…ƒä»¶é¡¯ç¤ºï¼Œè¦–è¦ºæ•ˆæœæœ€ä½³
            output_report = gr.Markdown(label="ç”Ÿæˆçš„å ±å‘Šå…§å®¹")
            
    # ç¶å®šäº‹ä»¶
    btn_submit.click(
        fn=generate_report,
        inputs=[input_case],
        outputs=[output_report]
    )

if __name__ == "__main__":
    print("å•Ÿå‹•ç¶²é ä»‹é¢...")
    demo.launch(server_name="0.0.0.0", server_port=7860)
