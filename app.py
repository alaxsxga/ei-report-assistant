import gradio as gr
import chromadb
import requests
import json
import os
import re
import sys
import anthropic
from dotenv import load_dotenv

# è¼‰å…¥ .env æª”æ¡ˆ
load_dotenv()

# åŠ å…¥ skill è·¯å¾‘ä»¥ä¾¿å°å…¥ prompt æ¨¡çµ„
SKILL_PATH = os.path.join(os.path.dirname(__file__), '.agent', 'skills', 'ot-report-generation')
sys.path.insert(0, SKILL_PATH)

# å°å…¥ prompt æ¨¡çµ„
from prompts import get_system_prompt, get_user_prompt

# ================= è¨­å®šå€ =================
# è³‡æ–™åº«è¨­å®š
DB_PATH = "./local_vector_db"
COLLECTION_NAME = "ot_reports"

# Ollama è¨­å®š (ç”¨æ–¼ Embedding å’Œç”Ÿæˆ)
OLLAMA_API_URL = "http://localhost:11434/api"
EMBEDDING_MODEL = "nomic-embed-text"  # å¿…é ˆèˆ‡å»ºç«‹è³‡æ–™åº«æ™‚ä¸€è‡´
GENERATION_MODEL = "gemma2"          # Google é–‹æºæ¨¡å‹ï¼Œé‚è¼¯æ€§å¼·ã€å›è¦†ä¹¾æ·¨

# Anthropic è¨­å®š
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
CLAUDE_MODEL = "claude-sonnet-4-20250514"
# =========================================

# 1. è³‡æ–™åº«é€£ç·šå‡½å¼
def get_chroma_collection():
    client = chromadb.PersistentClient(path=DB_PATH)
    return client.get_collection(COLLECTION_NAME)

# 2. Embedding å‡½å¼ (å°‡æ–‡å­—è½‰å‘é‡)
def get_embedding(text):
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/embeddings",
            json={"model": EMBEDDING_MODEL, "prompt": text},
            timeout=10
        )
        if response.status_code == 200:
            return response.json()["embedding"]
        else:
            print(f"Embedding Error: {response.text}")
            return None
    except Exception as e:
        print(f"Ollama Connection Error: {e}")
        return None

# 3. ç”Ÿæˆå›æ‡‰å‡½å¼ (RAG æ ¸å¿ƒé‚è¼¯)
def generate_report(case_description, model_choice):
    print(f"\n{'='*30}")
    print(f"ğŸš€ é–‹å§‹ç”Ÿæˆå ±å‘Šä»»å‹™")
    print(f"ğŸ¤– é¸æ“‡æ¨¡å‹: {model_choice}")
    if model_choice == "Claude 4 Sonnet (Cloud)":
        print(f"ğŸ“ ä½¿ç”¨ API æ¨¡å‹ ID: {CLAUDE_MODEL}")
    
    status_msg = "æ­£åœ¨åˆ†æè³‡æ–™..."
    yield status_msg
    
    # --- æ­¥é©Ÿ A: è§£æèˆ‡åˆ†å‰²å€å¡Š ---
    section_pattern = r'(?:\n|^)(?:\d+[\.ã€]\s*)?([\u4e00-\u9fa5]{2,6})(?:[:ï¼š]|\s|\n)([\s\S]*?)(?=(?:\n\d+[\.ã€]\s*|[\u4e00-\u9fa5]{2,6}[:ï¼š]|$))'
    sections = re.findall(section_pattern, case_description)
    print(f"ğŸ“‹ è§£æåˆ°å…§å®¹å€å¡Š: {[s[0] for s in sections] if sections else 'ç„¡(å…¨åŸŸæª¢ç´¢)'}")
    
    if not sections:
        query_tasks = [("ç¶œåˆæè¿°", case_description)]
    else:
        query_tasks = [(s[0].strip(), s[1].strip()) for s in sections if s[1].strip()]

    collection = get_chroma_collection()
    all_context_list = []
    
    status_msg += f"\næª¢æ¸¬åˆ° {len(query_tasks)} å€‹è©•ä¼°å€å¡Šï¼Œé–‹å§‹åˆ†å€æª¢ç´¢..."
    yield status_msg

    # --- æ­¥é©Ÿ B: é‡å°æ¯å€‹å€å¡Šé€²è¡Œå€‹åˆ¥æª¢ç´¢ ---
    for domain, content in query_tasks:
        print(f"ğŸ” æ­£åœ¨æª¢ç´¢é ˜åŸŸ: {domain}...")
        status_msg += f"\nğŸ” æª¢ç´¢ã€Œ{domain}ã€ç›¸é—œè³‡æ–™..."
        yield status_msg
        
        search_text = f"{domain}ï¼š{content}"
        embedding = get_embedding(search_text)
        
        if not embedding:
            print(f"âŒ ã€Œ{domain}ã€Embedding å¤±æ•—")
            continue
            
        results = collection.query(
            query_embeddings=[embedding],
            n_results=3 
        )
        
        if results['distances'] and results['documents']:
            found_count = 0
            for i, dist in enumerate(results['distances'][0]):
                similarity = 1.0 - dist
                if similarity > 0.6:
                    found_count += 1
                    doc = results['documents'][0][i]
                    all_context_list.append(f"ã€é‡å°ã€Œ{domain}ã€çš„æ­·å²åƒè€ƒè³‡æ–™ ({similarity:.2f})ã€‘\n{doc}\n")
            print(f"âœ… ã€Œ{domain}ã€æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° {found_count} ç­†ç›¸ä¼¼è³‡æ–™")
    
    
    if not all_context_list:
        context_str = "ï¼ˆâš ï¸ è­¦å‘Šï¼šæ‰€æœ‰å€å¡Šå‡æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ > 0.6 çš„æ¡ˆä¾‹ï¼Œä»¥ä¸‹å ±å‘Šå°‡åƒ…åŸºæ–¼ä¸€èˆ¬é‚è¼¯ç”Ÿæˆï¼‰"
        status_msg += "\nâš ï¸ æœªæ‰¾åˆ°é«˜ç›¸é—œæ¡ˆä¾‹ã€‚"
        retrieval_info = "\n\n---\n## ğŸ“‹ æª¢ç´¢çµæœ\n\næœªæ‰¾åˆ°ç›¸ä¼¼åº¦ > 0.6 çš„åƒè€ƒæ¡ˆä¾‹ã€‚\n\n---\n"
    else:
        context_str = "\n".join(all_context_list)
        status_msg += f"\nåˆ†å€æª¢ç´¢å®Œæˆï¼Œå…±æ”¶é›† {len(all_context_list)} ç­†é«˜ç›¸é—œåƒè€ƒè³‡æ–™ã€‚"
        
        # å»ºç«‹æª¢ç´¢çµæœæ‘˜è¦
        retrieval_info = "\n\n---\n## ğŸ“‹ æª¢ç´¢çµæœ\n\n"
        retrieval_info += f"**å…±æª¢ç´¢åˆ° {len(all_context_list)} ç­†åƒè€ƒè³‡æ–™ï¼š**\n\n"
        
        for idx, context in enumerate(all_context_list, 1):
            # æå–é ˜åŸŸå’Œç›¸ä¼¼åº¦
            lines = context.split('\n')
            header = lines[0] if lines else ""
            preview = '\n'.join(lines[1:6]) if len(lines) > 1 else ""  # é¡¯ç¤ºå‰5è¡Œ
            
            retrieval_info += f"### {idx}. {header}\n\n"
            retrieval_info += f"```\n{preview}\n...\n```\n\n"
        
        retrieval_info += "---\n\n## ğŸ¤– é–‹å§‹ç”Ÿæˆå ±å‘Š...\n\n"
    
    yield status_msg + retrieval_info
        
    yield status_msg + retrieval_info
        
    # --- æ­¥é©Ÿ C: ç”Ÿæˆ (Generation) ---
    print(f"ğŸ§  æº–å‚™é€²å…¥ LLM ç”Ÿæˆéšæ®µ...")
    system_prompt = get_system_prompt()
    user_prompt = get_user_prompt(context_str, case_description)

    if model_choice == "Claude 4 Sonnet (Cloud)":
        print(f"â˜ï¸ æ­£åœ¨å‘¼å« Anthropic Claude API...")
        # ä½¿ç”¨ Anthropic Claude ç”Ÿæˆ
        try:
            client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
            with client.messages.stream(
                model=CLAUDE_MODEL,
                max_tokens=4096,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            ) as stream:
                print("ğŸ“ Claude ä¸²æµé–‹å§‹æ¥æ”¶...")
                full_response = ""
                for text in stream.text_stream:
                    full_response += text
                    yield full_response
                print("âœ… Claude ç”Ÿæˆå®Œç•¢")
        except Exception as e:
            error_msg = f"Claude API éŒ¯èª¤: {str(e)}"
            print(f"âŒ {error_msg}")
            yield error_msg
    else:
        print(f"ğŸ  æ­£åœ¨å‘¼å«æœ¬åœ° Ollama ({GENERATION_MODEL})...")
        # å‘¼å« Ollama ç”Ÿæˆ (æ”¯æ´ä¸²æµé¡¯ç¤º)
        try:
            response = requests.post(
                f"{OLLAMA_API_URL}/chat",
                json={
                    "model": GENERATION_MODEL,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "stream": True # é–‹å•Ÿä¸²æµ
                },
                stream=True
            )

            full_response = ""
            for line in response.iter_lines():
                if line:
                    decoded_line = line.decode('utf-8')
                    try:
                        body = json.loads(decoded_line)
                        if "message" in body:
                            token = body["message"]["content"]
                            full_response += token
                            yield full_response
                        if body.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue
            print("âœ… Ollama ç”Ÿæˆå®Œç•¢")
        except Exception as e:
            error_msg = f"Ollama ç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(f"âŒ {error_msg}")
            yield error_msg

# ================= ä»‹é¢è¨­è¨ˆ (Gradio) =================
with gr.Blocks(title="AI è·èƒ½æ²»ç™‚å ±å‘ŠåŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¥ AI è·èƒ½æ²»ç™‚å ±å‘ŠåŠ©æ‰‹ (Local RAG)")
    gr.Markdown("è¼¸å…¥å€‹æ¡ˆçš„ä¸»è¨´èˆ‡è§€å¯Ÿï¼Œå°‡åƒè€ƒæ­·å²ç—…æ­·åº«ï¼Œç”Ÿæˆå•é¡Œåˆ†æèˆ‡å»ºè­°ã€‚")
    
    with gr.Row():
        with gr.Column(scale=1):
            input_case = gr.Textbox(
                label="ä¸»è¨´èˆ‡è©•ä¼°å…§å®¹", 
                placeholder="ä¾‹å¦‚ï¼šå®¶å±¬è¡¨ç¤ºå­©å­åœ¨å­¸æ ¡åä¸ä½ï¼Œå¯«å­—å¾ˆé†œ... è§€å¯Ÿç™¼ç¾æŠ“æ¡å§¿å‹¢ä¸æˆç†Ÿï¼Œç„¡æ³•å–®è…³ç«™ç«‹...",
                lines=12
            )
            model_radio = gr.Radio(
                choices=["Gemma2 (Local)", "Claude 4 Sonnet (Cloud)"],
                value="Claude 4 Sonnet (Cloud)",
                label="é¸æ“‡ç”Ÿæˆæ¨¡å‹"
            )
            api_key_input = gr.Textbox(
                label="Anthropic API Key (è‹¥ä½¿ç”¨ Claude)",
                placeholder="sk-...",
                type="password",
                visible=False
            )
            
            def toggle_api_input(choice):
                if choice == "Claude 3.5 Sonnet (Cloud)":
                    return gr.update(visible=True)
                return gr.update(visible=False)
                
            model_radio.change(fn=toggle_api_input, inputs=[model_radio], outputs=[api_key_input])

            btn_submit = gr.Button("ğŸ§  é–‹å§‹ç”Ÿæˆå ±å‘Š", variant="primary")
            
        with gr.Column(scale=1):
            # ä½¿ç”¨ Markdown å…ƒä»¶é¡¯ç¤ºï¼Œè¦–è¦ºæ•ˆæœæœ€ä½³
            output_report = gr.Markdown(label="ç”Ÿæˆçš„å ±å‘Šå…§å®¹")
            
    # ç¶å®šäº‹ä»¶
    def process_with_key(case, model, key):
        global ANTHROPIC_API_KEY
        if key:
            ANTHROPIC_API_KEY = key
        yield from generate_report(case, model)

    btn_submit.click(
        fn=lambda: gr.update(interactive=False, value="â³ æ­£åœ¨ç”Ÿæˆå ±å‘Š..."),
        outputs=[btn_submit]
    ).then(
        fn=process_with_key,
        inputs=[input_case, model_radio, api_key_input],
        outputs=[output_report]
    ).then(
        fn=lambda: gr.update(interactive=True, value="ğŸ§  é–‹å§‹ç”Ÿæˆå ±å‘Š"),
        outputs=[btn_submit]
    )

if __name__ == "__main__":
    print("å•Ÿå‹•ç¶²é ä»‹é¢...")
    demo.launch(server_name="0.0.0.0", server_port=7860)
