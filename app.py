import gradio as gr
import chromadb
import requests
import json
import os
import re
import sys

# åŠ å…¥ skill è·¯å¾‘ä»¥ä¾¿å°å…¥ prompt æ¨¡çµ„
SKILL_PATH = os.path.join(os.path.dirname(__file__), '.agent', 'skills', 'ot-report-generation')
sys.path.insert(0, SKILL_PATH)

# å°å…¥ prompt æ¨¡çµ„
from prompts import get_system_prompt, get_user_prompt

# ================= è¨­å®šå€ =================
# è³‡æ–™åº«è¨­å®š
DB_PATH = "./local_vector_db"
COLLECTION_NAME = "ot_reports"

# Ollama è¨­å®š (ç”¨æ–¼ Embedding å’Œç”Ÿæˆ)
OLLAMA_API_URL = "http://localhost:11434/api"
EMBEDDING_MODEL = "nomic-embed-text"  # å¿…é ˆèˆ‡å»ºç«‹è³‡æ–™åº«æ™‚ä¸€è‡´
GENERATION_MODEL = "qwen2.5:7b"      # ç”Ÿæˆç”¨çš„æ¨¡å‹ï¼Œå¯æ›æˆ llama3 æˆ–å…¶ä»–
# =========================================

# 1. è³‡æ–™åº«é€£ç·šå‡½å¼
def get_chroma_collection():
    client = chromadb.PersistentClient(path=DB_PATH)
    return client.get_collection(COLLECTION_NAME)

# 2. Embedding å‡½å¼ (å°‡æ–‡å­—è½‰å‘é‡)
def get_embedding(text):
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/embeddings",
            json={"model": EMBEDDING_MODEL, "prompt": text},
            timeout=10
        )
        if response.status_code == 200:
            return response.json()["embedding"]
        else:
            print(f"Embedding Error: {response.text}")
            return None
    except Exception as e:
        print(f"Ollama Connection Error: {e}")
        return None

# 3. ç”Ÿæˆå›æ‡‰å‡½å¼ (RAG æ ¸å¿ƒé‚è¼¯)
def generate_report(case_description):
    
    status_msg = "æ­£åœ¨åˆ†æè³‡æ–™..."
    yield status_msg # å›å‚³çµ¦å–®ä¸€è¼¸å‡ºæ¬„ä½
    
    # --- æ­¥é©Ÿ A: è§£æèˆ‡åˆ†å‰²å€å¡Š (Decomposed RAG) ---
    # åŒ¹é… "1. ç²¾ç´°å‹•ä½œ" æˆ– "ç²¾ç´°å‹•ä½œï¼š" ç­‰æ ¼å¼
    section_pattern = r'(?:\n|^)(?:\d+[\.ã€]\s*)?([\u4e00-\u9fa5]{2,6})(?:[:ï¼š]|\s|\n)([\s\S]*?)(?=(?:\n\d+[\.ã€]\s*|[\u4e00-\u9fa5]{2,6}[:ï¼š]|$))'
    sections = re.findall(section_pattern, case_description)
    
    if not sections:
        query_tasks = [("ç¶œåˆæè¿°", case_description)]
    else:
        query_tasks = [(s[0].strip(), s[1].strip()) for s in sections if s[1].strip()]

    collection = get_chroma_collection()
    all_context_list = []
    
    status_msg += f"\næª¢æ¸¬åˆ° {len(query_tasks)} å€‹è©•ä¼°å€å¡Šï¼Œé–‹å§‹åˆ†å€æª¢ç´¢..."
    yield status_msg

    # --- æ­¥é©Ÿ B: é‡å°æ¯å€‹å€å¡Šé€²è¡Œå€‹åˆ¥æª¢ç´¢ ---
    for domain, content in query_tasks:
        status_msg += f"\nğŸ” æª¢ç´¢ã€Œ{domain}ã€ç›¸é—œè³‡æ–™..."
        yield status_msg
        
        # å°‡é ˜åŸŸèˆ‡å…§å®¹åˆåœ¨ä¸€èµ·åšå‘é‡åŒ–ï¼Œå¢åŠ æœå°‹ç²¾æº–åº¦
        search_text = f"{domain}ï¼š{content}"
        embedding = get_embedding(search_text)
        
        if not embedding:
            continue

        # é‡å°å–®ä¸€é ˜åŸŸå–ç›¸ä¼¼åº¦æœ€é«˜çš„å‰ 3 ç­†ï¼Œé¿å… Token çˆ†ç‚¸
        results = collection.query(
            query_embeddings=[embedding],
            n_results=3 
        )
        
        if results['distances'] and results['documents']:
            for i, dist in enumerate(results['distances'][0]):
                similarity = 1.0 - dist
                if similarity > 0.6:
                    doc = results['documents'][0][i]
                    all_context_list.append(f"ã€é‡å°ã€Œ{domain}ã€çš„æ­·å²åƒè€ƒè³‡æ–™ ({similarity:.2f})ã€‘\n{doc}\n")
    
    if not all_context_list:
        context_str = "ï¼ˆâš ï¸ è­¦å‘Šï¼šæ‰€æœ‰å€å¡Šå‡æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ > 0.6 çš„æ¡ˆä¾‹ï¼Œä»¥ä¸‹å ±å‘Šå°‡åƒ…åŸºæ–¼ä¸€èˆ¬é‚è¼¯ç”Ÿæˆï¼‰"
        status_msg += "\nâš ï¸ æœªæ‰¾åˆ°é«˜ç›¸é—œæ¡ˆä¾‹ã€‚"
    else:
        context_str = "\n".join(all_context_list)
        status_msg += f"\nåˆ†å€æª¢ç´¢å®Œæˆï¼Œå…±æ”¶é›† {len(all_context_list)} ç­†é«˜ç›¸é—œåƒè€ƒè³‡æ–™ã€‚ç”Ÿæˆå ±å‘Šä¸­..."
    
    yield status_msg
        
    # --- æ­¥é©Ÿ B: ç”Ÿæˆ (Generation) ---
    # ä½¿ç”¨ skill æ¨¡çµ„ä¸­çš„ prompt
    system_prompt = get_system_prompt()
    user_prompt = get_user_prompt(context_str, case_description)

    # å‘¼å« Ollama ç”Ÿæˆ (æ”¯æ´ä¸²æµé¡¯ç¤º)
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/chat",
            json={
                "model": GENERATION_MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": True # é–‹å•Ÿä¸²æµ
            },
            stream=True
        )

        full_response = ""
        for line in response.iter_lines():
            if line:
                # å¿…é ˆå…ˆ decode byte string
                decoded_line = line.decode('utf-8')
                try:
                    body = json.loads(decoded_line)
                    if "message" in body:
                        token = body["message"]["content"]
                        full_response += token
                        yield full_response # æ›´æ–°è¼¸å‡º
                    if body.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue
                    
    except Exception as e:
        err = f"ç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        yield err

# ================= ä»‹é¢è¨­è¨ˆ (Gradio) =================
with gr.Blocks(title="AI è·èƒ½æ²»ç™‚å ±å‘ŠåŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¥ AI è·èƒ½æ²»ç™‚å ±å‘ŠåŠ©æ‰‹ (Local RAG)")
    gr.Markdown("è¼¸å…¥å€‹æ¡ˆçš„ä¸»è¨´èˆ‡è§€å¯Ÿï¼Œå°‡åƒè€ƒæ­·å²ç—…æ­·åº«ï¼Œç”Ÿæˆå•é¡Œåˆ†æèˆ‡å»ºè­°ã€‚")
    
    with gr.Row():
        with gr.Column(scale=1):
            input_case = gr.Textbox(
                label="ä¸»è¨´èˆ‡è©•ä¼°å…§å®¹", 
                placeholder="ä¾‹å¦‚ï¼šå®¶å±¬è¡¨ç¤ºå­©å­åœ¨å­¸æ ¡åä¸ä½ï¼Œå¯«å­—å¾ˆé†œ... è§€å¯Ÿç™¼ç¾æŠ“æ¡å§¿å‹¢ä¸æˆç†Ÿï¼Œç„¡æ³•å–®è…³ç«™ç«‹...",
                lines=15
            )
            btn_submit = gr.Button("ğŸ§  é–‹å§‹ç”Ÿæˆå ±å‘Š", variant="primary")
            
        with gr.Column(scale=1):
            # ä½¿ç”¨ Markdown å…ƒä»¶é¡¯ç¤ºï¼Œè¦–è¦ºæ•ˆæœæœ€ä½³
            output_report = gr.Markdown(label="ç”Ÿæˆçš„å ±å‘Šå…§å®¹")
            
    # ç¶å®šäº‹ä»¶
    btn_submit.click(
        fn=lambda: gr.update(interactive=False, value="â³ æ­£åœ¨ç”Ÿæˆå ±å‘Š..."),
        outputs=[btn_submit]
    ).then(
        fn=generate_report,
        inputs=[input_case],
        outputs=[output_report]
    ).then(
        fn=lambda: gr.update(interactive=True, value="ğŸ§  é–‹å§‹ç”Ÿæˆå ±å‘Š"),
        outputs=[btn_submit]
    )

if __name__ == "__main__":
    print("å•Ÿå‹•ç¶²é ä»‹é¢...")
    demo.launch(server_name="0.0.0.0", server_port=7860)
